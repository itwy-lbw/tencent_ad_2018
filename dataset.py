#coding=utf-8
import pandas as pd
import numpy as np
import os
import codecs
import csv
#from sklearn.model_selection import train_test_split
from parms import *
import scipy.special as special
from scipy import sparse
import copy
from sklearn.preprocessing import OneHotEncoder,LabelEncoder
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.preprocessing import StandardScaler
class HyperParam(object):
    def __init__(self, alpha, beta):
        self.alpha = alpha
        self.beta = beta

    def sample_from_beta(self, alpha, beta, num, imp_upperbound):
        #产生样例数据
        sample = numpy.random.beta(alpha, beta, num)
        I = []
        C = []
        for click_ratio in sample:
            imp = random.random() * imp_upperbound
            #imp = imp_upperbound
            click = imp * click_ratio
            I.append(imp)
            C.append(click)
        return pd.Series(I), pd.Series(C)

    def update_from_data_by_FPI(self, tries, success, iter_num, epsilon):
        #更新策略
        for i in range(iter_num):
            new_alpha, new_beta = self.__fixed_point_iteration(tries, success, self.alpha, self.beta)
            if abs(new_alpha-self.alpha)<epsilon and abs(new_beta-self.beta)<epsilon:
                break
            self.alpha = new_alpha
            self.beta = new_beta

    def __fixed_point_iteration(self, tries, success, alpha, beta):
        #迭代函数
        sumfenzialpha = 0.0
        sumfenzibeta = 0.0
        sumfenmu = 0.0
        sumfenzialpha = (special.digamma(success+alpha) - special.digamma(alpha)).sum()
        sumfenzibeta = (special.digamma(tries-success+beta) - special.digamma(beta)).sum()
        sumfenmu = (special.digamma(tries+alpha+beta) - special.digamma(alpha+beta)).sum()

        return alpha*(sumfenzialpha/sumfenmu), beta*(sumfenzibeta/sumfenmu)
class DataSet():
    # DataSet(UF_VW,ADF,TRAINVAL,UF_CSV,RANDOMSTATE)
    def __init__(self,UF_VW,ADF,TRAINVAL,UF_CSV,TRAINVAL_MERGE,TEST_MERGE,TEST,\
                TRAINVALTEST_DENSE_X,TRAINVAL_SPARSE_X,TRAINVALTEST_DENSE_X_NAMES,\
                TRAINVAL_SPARSE_X_NAMES,TEST_SPARSE_X,TEST_SPARSE_X_NAMES,\
                USE_TINY=False,RANDOMSTATE=2018):
        '''random seed for fully reappearance'''
        self.RANDOMSTATE=RANDOMSTATE

        '''init files provided by tencent'''
        self.UF_VW=UF_VW
        self.ADF=ADF
        self.TRAINVAL=TRAINVAL 
        self.TEST=TEST

        '''generated by vw2csv'''
        self.UF_CSV=UF_CSV 

        '''generated by mergeFiles'''
        self.TRAINVAL_MERGE=TRAINVAL_MERGE 
        self.TEST_MERGE=TEST_MERGE 
        
        '''the flag of tiny dataset'''
        self.use_tiny=USE_TINY
        '''make the data dataset'''
        self.makeDataset()   
        '''make folders'''
        self.makeFolders()
        
        '''paths for saving different features'''
        self.denseFeature_path=TRAINVALTEST_DENSE_X
        self.trainval_sparseFeature_path=TRAINVAL_SPARSE_X
        self.denseFeature_names_path=TRAINVALTEST_DENSE_X_NAMES
        self.trainval_sparseFeature_names_path=TRAINVAL_SPARSE_X_NAMES
        self.test_sparseFeature_path=TEST_SPARSE_X
        self.test_sparseFeature_names_path=TEST_SPARSE_X_NAMES
        self.n_trainval=len(pd.read_csv(TRAINVAL))  
    def vw2csv(self):
        '''change the vw format to csv format'''
        def to_dict(line):
            """
            file line to dict
            :param line:
            :return:
            """
            result = dict()
            fields = line.split('|')
            for field in fields:
                result[field.split()[0]] = ' '.join(field.split()[1:])
            return result
        print('vw2csv start!!!!')
        if not os.path.exists(self.UF_CSV):
            assert os.path.exists(self.UF_VW)

            head = ['uid', 'age', 'gender', 'marriageStatus', 'education', \
	            'consumptionAbility', 'LBS', 'interest1', 'interest2', 'interest3', \
	            'interest4', 'interest5', 'kw1', 'kw2', 'kw3', \
	            'topic1', 'topic2', 'topic3', 'appIdInstall','appIdAction',
	            'ct', 'os', 'carrier', 'house']
            file = codecs.open(self.UF_VW, encoding='utf-8')

            with open(self.UF_CSV, 'w') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=head)
                writer.writeheader()
                for i, line in enumerate(file):
	                # results.append(to_dict(line))
	                writer.writerow(to_dict(line))
	                if i % 10000 == 0:
	                    print('converted line ' + str(i))
            print('vw2csv done!')
        else:
            print('the csv format of userFeature has exist! skipping vw2csv!!!!')

    def mergeFiles(self,fillNa=False,na='-1',changeLabel=True):
        '''merger the all the files of trainval and test togethor!'''
        print('mergeTRVATE started!')
        if not (os.path.exists(self.TRAINVAL_MERGE) and os.path.exists(self.TEST_MERGE)):
            trainval=pd.read_csv(self.TRAINVAL)
            #print self.TEST

            test=pd.read_csv(self.TEST)
            n_tv=len(trainval)
            userFeature=pd.read_csv(self.UF_CSV)
            adFeature=pd.read_csv(self.ADF)
            if changeLabel:
                trainval.loc[trainval['label']==-1,'label']=0
            test['label']=-1
            data=pd.concat([trainval,test])
            data=pd.merge(data,adFeature,on='aid',how='left')
            data=pd.merge(data,userFeature,on='uid',how='left')
            if fillNa:
                data=data.fillna(na)
            if self.use_tiny and not os.path.exists(self.TRAINVAL_MERGE):
                df_positive=data[data['label']==1]
                df_negtive=data[data['label']==0]
                df_negtive_selected=df_negtive.sample(frac=0.01,random_state=self.RANDOMSTATE)
                df_positive_nodup=df_positive.drop_duplicates(['aid'])
                df_positive_selected=df_positive.sample(frac=0.01,random_state=self.RANDOMSTATE)
                pd.concat([df_positive_nodup,df_negtive_selected,df_positive_selected]).\
                            drop_duplicates().sample(frac=1.0,random_state=self.RANDOMSTATE)\
                            .to_csv(self.TRAINVAL_MERGE,index=False)
                print('create tiny Trainvalmerge sucess!!')
            elif not os.path.exists(self.TRAINVAL_MERGE):
                data.iloc[:n_tv].to_csv(self.TRAINVAL_MERGE,index=False)
                print('create normal Trainvalmerge sucess!!')
            else:
                print('Trainvalmerge already exist!!!!')
            if not os.path.exists(self.TEST_MERGE):
                data.iloc[n_tv:].drop(['label'],axis=1).to_csv(self.TEST_MERGE,index=False)
                print('create normal Testmerge sucess!!')
            else:
                print('Testmerge already exist!!!!')
            print('mergeFiles has been done!')
        else:
            print('There is no need to mergeFiles')
    def makeDataset(self):
        '''make the datasets'''
        if self.use_tiny:
            print('make tiny datasets start!')
        else:
            print('make normal datasets start!')   
        if not os.path.exists(self.UF_CSV):
            self.vw2csv()
        #print(os.path.exists(self.TRAINVAL_MERGE),os.path.exists(self.TEST_MERGE),self.TRAINVAL_MERGE)
        if not (os.path.exists(self.TRAINVAL_MERGE) and os.path.exists(self.TEST_MERGE)):
            self.mergeFiles(fillNa=True)           
        print('makeDataset has been Done!!!')
        self.trainval_y=pd.read_csv(self.TRAINVAL_MERGE)['label']

    '''=================================================================================================================='''
    '''all the methods above are used by __init__,the only output used by below is the TRAIN_MERGE(n,t) and TEST_MERGE(t)'''
    def load_TrainVal(self):
        '''used for fully-set training for normal dataset,and cv for tiny dataset'''
        trainval=pd.read_csv(self.TRAINVAL_MERGE)
        #train_X, val_X, train_y, val_y
        #print(trainval.dtypes[18:29])
        return trainval
    def load_Train_Val(self,test_frac=0.1):
        trainval=self.load_TrainVal()
        #train_X, val_X, train_y, val_y
        #print(trainval.dtypes[18:29])
        return train_test_split(trainval.drop(['label'],axis=1),\
                trainval['label'],test_size=test_frac, random_state=self.RANDOMSTATE)
    def load_Test(self):
        test=pd.read_csv(self.TEST_MERGE)
        #print(test.dtypes[28:29])
        return test
    def load_Train_Val_Test(self):
        '''used for training on normal dataset, resist overfitting!'''
        '''return value (train_X, val_X, train_y, val_y),test_x'''
        return self.load_Train_Val(),self.load_Test()
    def load_TrainVal_Test(self):
        return self.load_TrainVal(),self.load_Test()
    def load_dataset(self):
        '''api for most situations'''
        if self.use_tiny:
            self.trainval=self.load_TrainVal()
            self.data=self.trainval
            
            #return self.load_TrainVal_Test()
        else:
            self.trainval,self.test=self.load_TrainVal_Test()
            self.test['label']=-1
            self.data=pd.concat([self.trainval,self.test],ignore_index=True,sort=False)
            #return self.load_TrainVal_Test()
        self.n_trainval=len(self.trainval)
    def makeFolders(self):
        if not os.path.exists('tmp'):
            os.mkdir('tmp')
        if not os.path.exists('submissions'):
            os.mkdir('submissions')
        if not os.path.exists('features'):
            os.mkdir('features')
        if not os.path.exists('models'):
            os.mkdir('models')
    def nlp_feature_score(self,columns):
        '''used by dense_feature_engineer to convert sparse feature into densefeature'''
        print('starting nlp_feature_score!!!!!!!')
        data=pd.DataFrame()
        for feature in columns:
            feature_count = {}
            feature_label_count = {}
            feature_list = self.trainval[feature].astype('str').values.tolist()
            label_list = self.trainval['label'].tolist()
            for i in range(self.trainval.shape[0]):
                for item in feature_list[i].split(" "):
                    if item in feature_count.keys():
                        feature_count[item] += 1
                    else:
                        feature_count[item] = 1
                    if (item not in feature_label_count.keys()) and (label_list[i]==1):
                        feature_label_count[item] = 1
                    elif (item in feature_label_count.keys()) and (label_list[i]==1):
                        feature_label_count[item] += 1
            dianji=[]
            zhuanhua = []
            for item in feature_label_count.keys():
                dianji.append(feature_count[item])
                zhuanhua.append(feature_label_count[item])
            c = pd.DataFrame({'dianji':dianji,'zhuanhua':zhuanhua})
            hyper = HyperParam(1, 1)
            hyper.update_from_data_by_FPI(c['dianji'], c['zhuanhua'], 1000, 0.00000001)
            train_feature_score = []
            for i in range(self.trainval.shape[0]):
                score = 1
                for item in feature_list[i].split(" "):
                    if item not in feature_label_count.keys():
                        score += 0
                    else:
                        score  = score *(1- (feature_label_count[item]+hyper.alpha)/(feature_count[item]+hyper.alpha+hyper.beta))
                train_feature_score.append(score)
            #train[feature + "_score"] = train_feature_score
            #train[feature + "_score"][self.trainval[feature]=="-1"]=-1.0
            if not self.use_tiny:
                test_feature_list = self.test[feature].astype('str').values.tolist()
                for i in range(self.test.shape[0]):
                    score = 1
                    for item in test_feature_list[i].split(" "):
                        if item not in feature_label_count.keys():
                            score += 0
                        else:
                            score  = score *(1- (feature_label_count[item]+hyper.alpha)/\
                                     (feature_count[item]+hyper.alpha+hyper.beta))
                    train_feature_score.append(score)
            data[feature + "_score"] = train_feature_score
            data[feature + "_score"][self.data[feature].values=="-1"]=-1.0
        return data

    #def cv(self,fold_n=5):
    def denseFeature_engineering_test(self,nlp_columns,keep_columns,norm=False):
        '''feature_engineering for dense features'''
        print('denseFeature_engineering starting')
        #print(self.denseFeature_path+self.name+'.npy')

        data=self.data.copy()
        data_clicked = self.trainval[self.trainval['label'] == 1]
        '''------------------------------------------------------'''

        num_age= self.trainval['age'].value_counts().sort_index()
        num_age_clicked = data_clicked['age'].value_counts().sort_index()

        ratio_num_age = num_age_clicked / num_age

        ratio_num_age = pd.DataFrame({
            'age': ratio_num_age.index,
            'ratio_num_age' : ratio_num_age.values
        })
        data = pd.merge(data, ratio_num_age, on=['age'], how='left')

        num_gender= self.trainval['gender'].value_counts().sort_index()
        num_gender_clicked = data_clicked['gender'].value_counts().sort_index()

        ratio_num_gender = num_gender_clicked / num_gender

        ratio_num_gender = pd.DataFrame({
            'gender': ratio_num_gender.index,
            'ratio_num_gender' : ratio_num_gender.values
        })
        data = pd.merge(data, ratio_num_gender, on=['gender'], how='left')
        num_marry= self.trainval['marriageStatus'].value_counts().sort_index()
        num_marry_clicked = data_clicked['marriageStatus'].value_counts().sort_index()

        ratio_num_marry = num_marry_clicked / num_marry

        ratio_num_marry = pd.DataFrame({
            'marriageStatus': ratio_num_marry.index,
            'ratio_num_marry' : ratio_num_marry.values
        })
        data = pd.merge(data, ratio_num_marry, on=['marriageStatus'], how='left')

        num_adver= self.trainval['advertiserId'].value_counts().sort_index()
        num_adver_clicked = data_clicked['advertiserId'].value_counts().sort_index()

        ratio_num_adver = num_adver_clicked / num_adver

        ratio_num_adver = pd.DataFrame({
            'advertiserId': ratio_num_adver.index,
            'ratio_num_adver' : ratio_num_adver.values
        })
        data = pd.merge(data, ratio_num_adver, on=['advertiserId'], how='left')

        num_proI= self.trainval['productId'].value_counts().sort_index()
        num_proI_clicked = data_clicked['productId'].value_counts().sort_index()

        ratio_num_proI = num_proI_clicked / num_proI

        ratio_num_proI = pd.DataFrame({
            'productId': ratio_num_proI.index,
            'ratio_num_proI' : ratio_num_proI.values
        })
        data = pd.merge(data, ratio_num_proI, on=['productId'], how='left')
        num_proT= self.trainval['productType'].value_counts().sort_index()
        num_proT_clicked = data_clicked['productType'].value_counts().sort_index()

        ratio_num_proT = num_proT_clicked / num_proT

        ratio_num_proT = pd.DataFrame({
            'productType': ratio_num_proT.index,
            'ratio_num_proT' : ratio_num_proT.values
        })
        data = pd.merge(data, ratio_num_proT, on=['productType'], how='left')

        num_coms= self.trainval['consumptionAbility'].value_counts().sort_index()
        num_coms_clicked = data_clicked['consumptionAbility'].value_counts().sort_index()

        ratio_num_coms = num_coms_clicked / num_coms

        ratio_num_coms = pd.DataFrame({
            'consumptionAbility': ratio_num_coms.index,
            'ratio_num_coms' : ratio_num_coms.values
        })
        data = pd.merge(data, ratio_num_coms, on=['consumptionAbility'], how='left')
        '''------------------------------------------------------------'''
        print('开始加入广告点击率特征')

        num_ad = self.trainval['aid'].value_counts().sort_index()
        num_ad_clicked = data_clicked['aid'].value_counts().sort_index()

        ratio = num_ad_clicked / num_ad

        ratio_clicked = pd.DataFrame({
            'aid': ratio.index,
            'ratio_clicked' : ratio.values
        })
        data = pd.merge(data, ratio_clicked, on=['aid'], how='left')


        # 加入推广计划转化率

        print('开始加入推广计划转化率特征')

        num_campaign = self.trainval['campaignId'].value_counts().sort_index()
        num_campaign_clicked = data_clicked['campaignId'].value_counts().sort_index()
        ratio_num_campaign = num_campaign_clicked / num_campaign
        ratio_num_campaign = pd.DataFrame({
            'campaignId': ratio_num_campaign.index,
            'ratio_num_campaign' : ratio_num_campaign.values
        })

        data = pd.merge(data, ratio_num_campaign, on=['campaignId'], how='left')


        # 加入用户所在LBS的历史点击率

        print('开始加入加入用户所在LBS的历史点击率特征')
        num_lbs = self.trainval.groupby('LBS').uid.nunique()
        num_lbs_clicked = data_clicked.groupby('LBS').uid.nunique()
        ratio_num_lbs = num_lbs_clicked / num_lbs
        ratio_num_lbs = ratio_num_lbs.fillna(0)
        ratio_num_lbs = pd.DataFrame({
            'LBS': ratio_num_lbs.index,
            'ration_num_LBS' : ratio_num_lbs.values
        })
        data = pd.merge(data, ratio_num_lbs, on=['LBS'], how='left')
        # 增加每个广告推送给不同的用户数

        print('开始加入广告推送给不同用户的数特征')

        num_advertise_touser = self.trainval.groupby('aid').uid.nunique()
        num_advertise_touser = pd.DataFrame({
            'aid': num_advertise_touser.index,
            'num_advertise_touser' : num_advertise_touser.values
        })
        data = pd.merge(data, num_advertise_touser, on=['aid'], how='left')

        '''-----------------------------------------------------------------------------------------'''
        print('开始加入学历所对应转化率特征')

        num_education = self.trainval['education'].value_counts().sort_index()
        num_education_clicked = data_clicked['education'].value_counts().sort_index()
        ration_num_education = num_education_clicked / num_education
        ration_num_education = pd.DataFrame({
            'education': ration_num_education.index,
            'ration_num_education' : ration_num_education.values
        })
        data = pd.merge(data, ration_num_education, on=['education'], how='left')
        # 分布统计特征
        aid_age_count = data.groupby(['aid', 'age']).size().reset_index().rename(columns={0: 'aid_age_count'})
        data = pd.merge(data, aid_age_count, 'left', on=['aid', 'age'])
        aid_gender_count = data.groupby(['aid', 'gender']).size().reset_index().rename(columns={0: 'aid_gender_count'})
        data = pd.merge(data, aid_gender_count, 'left', on=['aid', 'gender'])

        # 活跃特征
        add = pd.DataFrame(data.groupby(["campaignId"]).aid.nunique()).reset_index()
        add.columns = ["campaignId", "campaignId_active_aid"]
        data = data.merge(add, on=["campaignId"], how="left")
        data=pd.concat([data,self.nlp_feature_score(nlp_columns)],axis=1)
        print('data shape:',data[keep_columns].shape)
        if norm:
            data[keep_columns] = data[keep_columns].apply(lambda x: (x - np.mean(x)) / (np.std(x)))
        self.save_data(self.denseFeature_path,data[keep_columns].values,False)
        self.save_data(self.denseFeature_names_path,np.array(keep_columns),False)
        return data[keep_columns].values,np.array(keep_columns)
    def denseFeature_engineering(self,nlp_columns,keep_columns,norm=False):
        '''feature_engineering for dense features'''
        print('denseFeature_engineering starting')
        #print(self.denseFeature_path+self.name+'.npy')

        data=self.data.copy()
        data_clicked = self.trainval[self.trainval['label'] == 1]
        print('开始加入广告点击率特征')

        num_ad = self.trainval['aid'].value_counts().sort_index()
        num_ad_clicked = data_clicked['aid'].value_counts().sort_index()

        ratio = num_ad_clicked / num_ad

        ratio_clicked = pd.DataFrame({
            'aid': ratio.index,
            'ratio_clicked' : ratio.values
        })
        data = pd.merge(data, ratio_clicked, on=['aid'], how='left')


        # 加入推广计划转化率

        print('开始加入推广计划转化率特征')

        num_campaign = self.trainval['campaignId'].value_counts().sort_index()
        num_campaign_clicked = data_clicked['campaignId'].value_counts().sort_index()
        ratio_num_campaign = num_campaign_clicked / num_campaign
        ratio_num_campaign = pd.DataFrame({
            'campaignId': ratio_num_campaign.index,
            'ratio_num_campaign' : ratio_num_campaign.values
        })

        data = pd.merge(data, ratio_num_campaign, on=['campaignId'], how='left')


        # 加入用户所在LBS的历史点击率

        print('开始加入加入用户所在LBS的历史点击率特征')
        num_lbs = self.trainval.groupby('LBS').uid.nunique()
        num_lbs_clicked = data_clicked.groupby('LBS').uid.nunique()
        ratio_num_lbs = num_lbs_clicked / num_lbs
        ratio_num_lbs = ratio_num_lbs.fillna(0)
        ratio_num_lbs = pd.DataFrame({
            'LBS': ratio_num_lbs.index,
            'ration_num_LBS' : ratio_num_lbs.values
        })
        data = pd.merge(data, ratio_num_lbs, on=['LBS'], how='left')
        # 增加每个广告推送给不同的用户数

        print('开始加入广告推送给不同用户的数特征')

        num_advertise_touser = self.trainval.groupby('aid').uid.nunique()
        num_advertise_touser = pd.DataFrame({
            'aid': num_advertise_touser.index,
            'num_advertise_touser' : num_advertise_touser.values
        })
        data = pd.merge(data, num_advertise_touser, on=['aid'], how='left')

        '''-----------------------------------------------------------------------------------------'''
        print('开始加入学历所对应转化率特征')

        num_education = self.trainval['education'].value_counts().sort_index()
        num_education_clicked = data_clicked['education'].value_counts().sort_index()
        ration_num_education = num_education_clicked / num_education
        ration_num_education = pd.DataFrame({
            'education': ration_num_education.index,
            'ration_num_education' : ration_num_education.values
        })
        data = pd.merge(data, ration_num_education, on=['education'], how='left')
        # 分布统计特征
        aid_age_count = data.groupby(['aid', 'age']).size().reset_index().rename(columns={0: 'aid_age_count'})
        data = pd.merge(data, aid_age_count, 'left', on=['aid', 'age'])
        aid_gender_count = data.groupby(['aid', 'gender']).size().reset_index().rename(columns={0: 'aid_gender_count'})
        data = pd.merge(data, aid_gender_count, 'left', on=['aid', 'gender'])

        # 活跃特征
        add = pd.DataFrame(data.groupby(["campaignId"]).aid.nunique()).reset_index()
        add.columns = ["campaignId", "campaignId_active_aid"]
        data = data.merge(add, on=["campaignId"], how="left")
        data=pd.concat([data,self.nlp_feature_score(nlp_columns)],axis=1)
        print('data shape:',data[keep_columns].shape)
        if norm:
            data[keep_columns] = data[keep_columns].apply(lambda x: (x - np.mean(x)) / (np.std(x)))
        self.save_data(self.denseFeature_path,data[keep_columns].values,False)
        self.save_data(self.denseFeature_names_path,np.array(keep_columns),False)
        return data[keep_columns].values,np.array(keep_columns)


    def sparseFeature_engineering(self,one_hot_feature,vector_feature,min_df=1,n_gram=(1,2),norm=False):
        '''feature_engineering for sparse features'''
        #print(one_hot_feature,vector_feature,one_hot_feature.extend(vector_feature))
        #print('sparseFeature_engineering starting')
        #print(one_hot_feature,vector_feature,one_hot_feature.extend(vector_feature))
        keeped_feature=copy.copy(one_hot_feature)
        keeped_feature.extend(vector_feature)
        data=self.data[keeped_feature].copy()

        #print data.shape
        for feature in one_hot_feature:
            try:
                data[feature] = LabelEncoder().fit_transform(data[feature].apply(int))
            except:
                data[feature] = LabelEncoder().fit_transform(data[feature])
        trainval=data[:self.n_trainval]
        test=data[self.n_trainval:]
        enc=OneHotEncoder()
        train_x=None
        test_x=None
        for feature in one_hot_feature:
            #print feature
            enc.fit(data[feature].values.reshape(-1, 1))
            train_a=enc.transform(trainval[feature].values.reshape(-1, 1))
            if train_x is not None:
                train_x= sparse.hstack((train_x, train_a))
            else:
                train_x=train_a
            
            if not self.use_tiny:
                test_a = enc.transform(test[feature].values.reshape(-1,1))
                if test_x is not None:
                    test_x = sparse.hstack((test_x, test_a))
                else:
                    test_x=test_a
        print('one-hot prepared !')
        cv=CountVectorizer(min_df=min_df,ngram_range=n_gram)
        for feature in vector_feature:
            cv.fit(data[feature])
            train_a = cv.transform(trainval[feature])
            if train_x is not None:
                train_x= sparse.hstack((train_x, train_a))
            else:
                train_x=train_a
            if not self.use_tiny:
                test_a = cv.transform(test[feature])
                if test_x is not None:
                    test_x = sparse.hstack((test_x, test_a))
                else:
                    test_x=test_a
        scaler1 = StandardScaler(with_mean=False)
        scaler2 = StandardScaler(with_mean=False)
        print('countvectorizer prepared !')
        #if norm:
        #    train_x=scaler1.fit_transform(train_x)
        self.save_data(self.trainval_sparseFeature_path,train_x,True)
        self.save_data(self.trainval_sparseFeature_names_path,np.array(keeped_feature),False)
        if not self.use_tiny:
            #if norm:
            #    test_x=scaler1.fit_transform(test_x)
            self.save_data(self.test_sparseFeature_path,test_x,True)
            self.save_data(self.test_sparseFeature_names_path,np.array(keeped_feature),False)
            return train_x,test_x
        return train_x
    def sparseFeature_engineering2(self,one_hot_feature,vector_feature,min_df=1,n_gram=(1,2)):
        '''feature_engineering for sparse features'''
        #print(one_hot_feature,vector_feature,one_hot_feature.extend(vector_feature))
        #print('sparseFeature_engineering starting')
        #print(one_hot_feature,vector_feature,one_hot_feature.extend(vector_feature))
        keeped_feature=copy.copy(one_hot_feature)
        keeped_feature.extend(vector_feature)
        data=self.data[keeped_feature].copy()
        '''----------------------'''
        def add_prefix(line,col_name):
            line=str(line)
            def concat_pre(term):
                return col_name+'_'+str(term)
            if line=='-1':
                return '-1'
            else:
                return ' '.join(map(concat_pre,line.split(' ')))

        '''----------------------'''
        data['aaaaa']=''
        for feature in one_hot_feature:
            #print feature
            data[feature]=data[feature].apply(add_prefix,args =(feature,))
            data['aaaaa']+=data[feature]
        #print data.shape
        #for feature in one_hot_feature:
        #    try:
        #        data[feature] = LabelEncoder().fit_transform(data[feature].apply(int))
        #    except:
        #        data[feature] = LabelEncoder().fit_transform(data[feature])
        trainval=data[:self.n_trainval]
        test=data[self.n_trainval:]
        #enc=OneHotEncoder()
        train_x=None
        test_x=None

        print('one-hot prepared !')
        cv=CountVectorizer(min_df=min_df,ngram_range=n_gram)
        vector_feature.append('aaaaa')
        for feature in vector_feature:
            cv.fit(data[feature])
            train_a = cv.transform(trainval[feature])
            if train_x is not None:
                train_x= sparse.hstack((train_x, train_a))
            else:
                train_x=train_a
            if not self.use_tiny:
                test_a = cv.transform(test[feature])
                if test_x is not None:
                    test_x = sparse.hstack((test_x, test_a))
                else:
                    test_x=test_a
        print('countvectorizer prepared !')
        self.save_data(self.trainval_sparseFeature_path,train_x,True)
        self.save_data(self.trainval_sparseFeature_names_path,np.array(keeped_feature),False)
        if not self.use_tiny:
            self.save_data(self.test_sparseFeature_path,test_x,True)
            self.save_data(self.test_sparseFeature_names_path,np.array(keeped_feature),False)
            return train_x,test_x
        return train_x
    def load_features(self,files):
        data=map(self.load_data,files)
        if self.use_tiny:
            return sparse.hstack((data[0],data[1]))
        else: 
            return sparse.hstack((data[0][:self.n_trainval],data[1])),sparse.hstack((data[0][self.n_trainval:],data[2]))       
    def attempt_to_load_features(self):
        #fea_file_all_exist=True
        files=self.feature_group_paths()
        #print files
        #if self.feature_group_name:
        #    self.load_dataset()
        #    return False
        for f in files:
            if not os.path.exists(f):
                self.load_dataset()
                return False
        return self.load_features(files)
        
    def feature_engineering(self,feature_group_name='base'):
        '''api for all baseclassifiery'''
        '''must use load_data first!!!!!!!!!!!'''
        print('starting feature_engineering-{}!!!!'.format(feature_group_name))
        self.feature_group_name=feature_group_name
        rs=self.attempt_to_load_features()
        if rs is not False:
            print('skipping the feature_engineering,just load!')
            return rs
        else:
            print('rss',rs)
        if feature_group_name=='base':
            columns=['creativeSize',"aid_age_count",'aid_gender_count',\
                    'campaignId_active_aid','appIdAction_score','num_advertise_touser',\
                    'ratio_clicked','ratio_num_campaign','interest1_score',\
                    'interest2_score','interest5_score','interest3_score','interest4_score',\
                    'ration_num_education']
            nlp_columns=['appIdAction','interest1','interest2','interest3','interest4','interest5']
            one_hot_features=['LBS','age','carrier','consumptionAbility','education','gender',\
                            'house','os','ct','marriageStatus','advertiserId','campaignId',\
                            'creativeId','adCategoryId', 'productId', 'productType']
            vector_features=['appIdAction','appIdInstall','interest1','interest2',
                    'interest3','interest4','interest5','kw1','kw2','kw3','topic1','topic2','topic3']
            data_dense_x,_=self.denseFeature_engineering(nlp_columns,columns)
            if not self.use_tiny:
                trainval_sparse_x,test_sparse_x=self.sparseFeature_engineering(one_hot_features,vector_features)
                trainval_x=sparse.hstack([data_dense_x[:self.n_trainval],trainval_sparse_x])
                test_x=sparse.hstack([data_dense_x[self.n_trainval:],test_sparse_x])
                return trainval_x,test_x
            else:
                trainval_sparse_x=self.sparseFeature_engineering(one_hot_features,vector_features)
                trainval_x=sparse.hstack([data_dense_x[:self.n_trainval],trainval_sparse_x])
                return trainval_x
        elif feature_group_name=='base2':
            columns=['creativeSize',"aid_age_count",'aid_gender_count',\
                    'campaignId_active_aid','appIdAction_score','num_advertise_touser',\
                    'ratio_clicked','ratio_num_campaign','interest1_score',\
                    'interest2_score','interest5_score','interest3_score','interest4_score',\
                    'ration_num_education']
            nlp_columns=['appIdAction','interest1','interest2','interest3','interest4','interest5']
            one_hot_features=['LBS','age','carrier','consumptionAbility','education','gender',\
                            'house','os','ct','marriageStatus','advertiserId','campaignId',\
                            'creativeId','adCategoryId', 'productId', 'productType']
            vector_features=['appIdAction','appIdInstall','interest1','interest2',
                    'interest3','interest4','interest5','kw1','kw2','kw3','topic1','topic2','topic3']
            data_dense_x,_=self.denseFeature_engineering(nlp_columns,columns)
            if not self.use_tiny:
                trainval_sparse_x,test_sparse_x=self.sparseFeature_engineering(one_hot_features,vector_features,0.0009,(1,2))
                trainval_x=sparse.hstack([data_dense_x[:self.n_trainval],trainval_sparse_x])
                test_x=sparse.hstack([data_dense_x[self.n_trainval:],test_sparse_x])
                return trainval_x,test_x
            else:
                trainval_sparse_x=self.sparseFeature_engineering(one_hot_features,vector_features,0.0009,(1,2))
                trainval_x=sparse.hstack([data_dense_x[:self.n_trainval],trainval_sparse_x])
                return trainval_x
        elif feature_group_name=='base3':
            columns=['creativeSize',"aid_age_count",'aid_gender_count',\
                    'campaignId_active_aid','appIdAction_score','num_advertise_touser',\
                    'ratio_clicked','ratio_num_campaign','interest1_score',\
                    'interest2_score','interest5_score','interest3_score','interest4_score',\
                    'ration_num_education']
            nlp_columns=['appIdAction','interest1','interest2','interest3','interest4','interest5']
            one_hot_features=['LBS','age','carrier','consumptionAbility','education','gender',\
                            'house','os','ct','marriageStatus','advertiserId','campaignId',\
                            'creativeId','adCategoryId', 'productId', 'productType']
            vector_features=['appIdAction','appIdInstall','interest1','interest2',
                    'interest3','interest4','interest5','kw1','kw2','kw3','topic1','topic2','topic3']
            data_dense_x,_=self.denseFeature_engineering(nlp_columns,columns)
            if not self.use_tiny:
                trainval_sparse_x,test_sparse_x=self.sparseFeature_engineering2(one_hot_features,vector_features,0.0009,(1,2))
                trainval_x=sparse.hstack([data_dense_x[:self.n_trainval],trainval_sparse_x])
                test_x=sparse.hstack([data_dense_x[self.n_trainval:],test_sparse_x])
                return trainval_x,test_x
            else:
                trainval_sparse_x=self.sparseFeature_engineering2(one_hot_features,vector_features,0.0009,(1,2))
                trainval_x=sparse.hstack([data_dense_x[:self.n_trainval],trainval_sparse_x])
                return trainval_x
        elif feature_group_name=='base_norm':
            columns=['creativeSize',"aid_age_count",'aid_gender_count',\
                    'campaignId_active_aid','appIdAction_score','num_advertise_touser',\
                    'ratio_clicked','ratio_num_campaign','interest1_score',\
                    'interest2_score','interest5_score','interest3_score','interest4_score',\
                    'ration_num_education']
            nlp_columns=['appIdAction','interest1','interest2','interest3','interest4','interest5']
            one_hot_features=['LBS','age','carrier','consumptionAbility','education','gender',\
                            'house','os','ct','marriageStatus','advertiserId','campaignId',\
                            'creativeId','adCategoryId', 'productId', 'productType']
            vector_features=['appIdAction','appIdInstall','interest1','interest2',
                    'interest3','interest4','interest5','kw1','kw2','kw3','topic1','topic2','topic3']
            data_dense_x,_=self.denseFeature_engineering(nlp_columns,columns,norm=True)
            if not self.use_tiny:
                trainval_sparse_x,test_sparse_x=self.sparseFeature_engineering(one_hot_features,vector_features,norm=True)
                trainval_x=sparse.hstack([data_dense_x[:self.n_trainval],trainval_sparse_x])
                test_x=sparse.hstack([data_dense_x[self.n_trainval:],test_sparse_x])
                return trainval_x,test_x
            else:
                trainval_sparse_x=self.sparseFeature_engineering(one_hot_features,vector_features,norm=True)
                trainval_x=sparse.hstack([data_dense_x[:self.n_trainval],trainval_sparse_x])
                return trainval_x
        elif feature_group_name=='test':
            columns=['creativeSize',"aid_age_count",'aid_gender_count',\
                    'campaignId_active_aid','appIdAction_score','num_advertise_touser',\
                    'ratio_clicked','ratio_num_campaign','interest1_score',\
                    'interest2_score','interest5_score','interest3_score','interest4_score',\
                    'ration_num_education','ratio_num_age','ratio_num_gender','ratio_num_adver',\
                    'ratio_num_proI','ratio_num_proT','ratio_num_coms']
            nlp_columns=['appIdAction','interest1','interest2','interest3','interest4','interest5']
            one_hot_features=['LBS','age','carrier','consumptionAbility','education','gender',\
                            'house','os','ct','marriageStatus','advertiserId','campaignId',\
                            'creativeId','adCategoryId', 'productId', 'productType']
            vector_features=['appIdAction','appIdInstall','interest1','interest2',
                    'interest3','interest4','interest5','kw1','kw2','kw3','topic1','topic2','topic3']
            data_dense_x,_=self.denseFeature_engineering_test(nlp_columns,columns)
            if not self.use_tiny:
                trainval_sparse_x,test_sparse_x=self.sparseFeature_engineering(one_hot_features,vector_features,min_df=0.0009)
                trainval_x=sparse.hstack([data_dense_x[:self.n_trainval],trainval_sparse_x])
                test_x=sparse.hstack([data_dense_x[self.n_trainval:],test_sparse_x])
                return trainval_x,test_x
            else:
                trainval_sparse_x=self.sparseFeature_engineering(one_hot_features,vector_features)
                trainval_x=sparse.hstack([data_dense_x[:self.n_trainval],trainval_sparse_x])
                return trainval_x
        else:
            raise('The feature group not defined!!!!')
    def save_data(self,path,data,isSparse):
        '''save data npy for dense and npz for sparse'''
        if isSparse:
            return sparse.save_npz(path+'_'+self.feature_group_name+'.npz',data)
        else:
            return np.save(path+'_'+self.feature_group_name+'.npy',data)
    def load_data(self,path):
        '''save data npy for dense and npz for sparse'''
        if path.split('.')[-1]=='npz':
            return sparse.load_npz(path)
        else:
            return np.load(path)  
    def feature_group_paths(self):
        '''
        self.denseFeature_path=TRAINVALTEST_DENSE_X
        self.trainval_sparseFeature_path=TRAINVAL_SPARSE_X
        self.denseFeature_names_path=TRAINVALTEST_DENSE_X_NAMES
        self.trainval_sparseFeature_names_path=TRAINVAL_SPARSE_X_NAMES
        self.test_sparseFeature_path=TEST_SPARSE_X
        self.test_sparseFeature_names_path=TEST_SPARSE_X_NAMES
        '''

        tvt_d_x=self.denseFeature_path+'_'+self.feature_group_name+'.npy'
        tv_s_x=self.trainval_sparseFeature_path+'_'+self.feature_group_name+'.npz'
        t_s_x=self.test_sparseFeature_path+'_'+self.feature_group_name+'.npz'
        if not self.use_tiny:
            return [tvt_d_x,tv_s_x,t_s_x]
        else:           
            return [tvt_d_x,tv_s_x]
if __name__ == '__main__':
    tiny_ds=DataSet(UF_VW,ADF,TRAINVAL,UF_CSV,TRAINVAL_MERGE_TINY,TEST_MERGE,TEST,\
                TRAINVALTEST_DENSE_X_TINY,TRAINVAL_SPARSE_X_TINY,TRAINVALTEST_DENSE_X_NAMES_TINY,\
                TRAINVAL_SPARSE_X_NAMES_TINY,TEST_SPARSE_X,TEST_SPARSE_X_NAMES,\
                USE_TINY=True,RANDOMSTATE=2018)
    tiny_ds.load_dataset()
    train_X=tiny_ds.feature_engineering('test')
    print type(train_X),train_X.shape
